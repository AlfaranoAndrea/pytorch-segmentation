{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEufh0mUaqHX",
        "outputId": "c87badce-1c8b-43c6-eaef-1e05536513cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.0.6-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.7.1)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-2.0.6 torchmetrics-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "1_DTHQaDa22V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        # Left side (contracting path)\n",
        "        self.dwn_conv1 = dual_conv(1, 64)\n",
        "        self.dwn_conv2 = dual_conv(64, 128)\n",
        "        self.dwn_conv3 = dual_conv(128, 256)\n",
        "        self.dwn_conv4 = dual_conv(256, 512)\n",
        "        self.dwn_conv5 = dual_conv(512, 1024)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        #Right side  (expnsion path)\n",
        "        #transpose convolution is used showna as green arrow in architecture image\n",
        "        self.trans1 = nn.ConvTranspose2d(1024,512, kernel_size=2, stride= 2)\n",
        "        self.up_conv1 = dual_conv(1024,512)\n",
        "        self.trans2 = nn.ConvTranspose2d(512,256, kernel_size=2, stride= 2)\n",
        "        self.up_conv2 = dual_conv(512,256)\n",
        "        self.trans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride= 2)\n",
        "        self.up_conv3 = dual_conv(256,128)\n",
        "        self.trans4 = nn.ConvTranspose2d(128,64, kernel_size=2, stride= 2)\n",
        "        self.up_conv4 = dual_conv(128,64)\n",
        "\n",
        "        #output layer\n",
        "        self.out = nn.Conv2d(64, 2, kernel_size=1)\n",
        "\n",
        "    def forward(self, image):\n",
        "\n",
        "        #forward pass for Left side\n",
        "        x1 = self.dwn_conv1(image)\n",
        "        x2 = self.maxpool(x1)\n",
        "        x3 = self.dwn_conv2(x2)\n",
        "        x4 = self.maxpool(x3)\n",
        "        x5 = self.dwn_conv3(x4)\n",
        "        x6 = self.maxpool(x5)\n",
        "        x7 = self.dwn_conv4(x6)\n",
        "        x8 = self.maxpool(x7)\n",
        "        x9 = self.dwn_conv5(x8)\n",
        "\n",
        "\n",
        "        #forward pass for Right side\n",
        "        x = self.trans1(x9)\n",
        "        y = crop_tensor(x, x7)\n",
        "        x = self.up_conv1(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.trans2(x)\n",
        "        y = crop_tensor(x, x5)\n",
        "        x = self.up_conv2(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.trans3(x)\n",
        "        y = crop_tensor(x, x3)\n",
        "        x = self.up_conv3(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.trans4(x)\n",
        "        y = crop_tensor(x, x1)\n",
        "        x = self.up_conv4(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TXyqqXNBa0is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "b47Z38hHY7Rl",
        "outputId": "d5d45fb4-2d6b-4755-aa1b-805017952374"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f635b0640d2a>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Training the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCityscapesSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f635b0640d2a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# U-Net with 3 input channels (RGB) and the number of classes in Cityscapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 34 classes in Cityscapes, can be adjusted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unet.__init__() takes 1 positional argument but 3 were given"
          ]
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CityscapesSegmentation(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        super(CityscapesSegmentation, self).__init__()\n",
        "\n",
        "        # U-Net with 3 input channels (RGB) and the number of classes in Cityscapes\n",
        "        self.model = Unet(3, 34)  # 34 classes in Cityscapes, can be adjusted\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Modify these paths and parameters as per your setup\n",
        "        data_dir = \"path_to_cityscapes_data\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize for faster training\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        dataset = Cityscapes(data_dir, split='train', mode='fine', target_type='semantic',\n",
        "                             transform=transform, target_transform=transforms.ToTensor())\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "        return dataloader\n",
        "\n",
        "# Training the Model\n",
        "if __name__ == '__main__':\n",
        "    model = CityscapesSegmentation()\n",
        "    trainer = pl.Trainer(max_epochs=10)\n",
        "    trainer.fit(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpgZD8TFab99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CityscapesSegmentation(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        super(CityscapesSegmentation, self).__init__()\n",
        "\n",
        "        # U-Net with 3 input channels (RGB) and the number of classes in Cityscapes\n",
        "        self.model = UNet(3, 34)  # 34 classes in Cityscapes, can be adjusted\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "        outputs = self(images)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Modify these paths and parameters as per your setup\n",
        "        data_dir = \"path_to_cityscapes_data\"\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Resize for faster training\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        dataset = Cityscapes(data_dir, split='train', mode='fine', target_type='semantic',\n",
        "                             transform=transform, target_transform=transforms.ToTensor())\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "        return dataloader\n",
        "\n",
        "# Training the Model\n",
        "if __name__ == '__main__':\n",
        "    model = CityscapesSegmentation()\n",
        "    trainer = pl.Trainer(max_epochs=10)\n",
        "    trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "b_IYRzdNaXJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DIOwsy6PanmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
